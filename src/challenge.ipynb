{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge - David Felipe Martinez Castiblanco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalidades\n",
    "\n",
    "A continuaci√≥n, presentar√© una serie de generalidades que aplican para todas las preguntas:\n",
    "\n",
    "##### Estilo de c√≥digo\n",
    "  - Us√© pylint y pycodestyle para aumentar la calidad de mi c√≥digo, sin embargo desactiv√© dos reglas:\n",
    "     - R0801: Existe c√≥digo duplicasdo entre archivos, la raz√≥n es que estoy usando estrategias an√°logas en los archivos, soy conciente que es una mala practica y el c√≥digo deberia ser mucho m√°s modular, sin embargo etend√≠ que el ejercicio no lo permite.\n",
    "     - W1514: Estoy abriendo archivos sin especificar su codificaci√≥n, la raz√≥n de desactivarlo es que no conozco la codificaci√≥n original del archivo, y no quiero depronto da√±ar mis calculos por una mala codificaci√≥, esta vez le dejo a python buscar la correcta.\n",
    "\n",
    "##### Estructura del Dataset:\n",
    "  - Es un dataset excesivamente grande en comparaci√≥n a lo que se requiere de este. Solamente se usaron 3 atributos de todos los que tiene cada muestra.\n",
    "  - Debido a que los atributos requeridos no tenian inconsistencias, no se hizo limpieza alguna del dataset, exceptuando la limpieza minima de cada ejercicio (pasar a mayusculas un usuario, extraer substrings de alg√∫n texto).\n",
    "  - Vi que varios tweets tenian otros tweets anidados (quoted), sin embargo hice la suposici√≥n de que no se tendr√°n en cuenta para los conteos, es decir, no est√°n incluidos en ninguna parte de mi ejercicio.\n",
    "\n",
    "##### Lectura de los datos:\n",
    "\n",
    "  - Estamos hablando de un archivo algo pesado. Si pensamos en cargarlo por completo en memoria, tendr√≠amos que aprovecharlo muy bien y empezar a paralelizar tareas. Este ser√≠a el caso cuando queremos optimizar el tiempo de ejecuci√≥n.\n",
    "  - Si tenemos los datos divididos desde un principio, podr√≠amos hacer que m√∫ltiples hilos abran cada partici√≥n y la procesen. En nuestro caso, tenemos un archivo completo, pero podemos cargarlo todo en memoria, particionarlo en chunks y hacer c√°lculos de manera paralela.\n",
    "     - Entend√≠ que el ejercicio no me permite usar m√°s funciones, pero en una futura mejora, veo obligatorio particionar el archivo y limpiarlo para dejar solo los atributos que se necesitan. Esto facilita y optimiza el proceso de trasnformaci√≥n y obtenci√≥n de metricas\n",
    "  - En los 3 ejercicios enfocados en tiempo de procesamiento, los llev√© hacia el enfoque map-reduce, y por medio de multihilo proces√© lo que se requer√≠a. S√© que no es tan √≥ptimo cuando se presentan pocos datos o un solo archivo, pero cuando la cantidad de datos tiende crece de manera masiva(big data), esta soluci√≥n superar√≠a por mucho al enfoque que di optimizando memoria.\n",
    "  - Por el contrario, si necesitamos optimizar el uso de memoria, podemos ir leyendo los datos l√≠nea a l√≠nea y calculando secuencialmente lo que se requiere. Esto se convierte en una tarea lineal, pero es una manera de cargar un archivo pesado optimizando el uso de memoria.\n",
    "  - Est√° claro que al escalar el problema, es decir, si tenemos x archivos del mismo o mayor tama√±o, esta soluci√≥n se vuelve insostenible. Por lo que debe escalar hacia algo enfocado en arquitectura big data. Python, por ejemplo, nos ofrece librer√≠as como MRJob para esto.\n",
    "  - Siempre es √∫til liberar memoria dinamicamente, es decir, borrar aquellas variables pesadas que se usan una sola vez.\n",
    "  - A gran escala, el cargue de los datos es tan importante como su procesamiento, y la nube tambi√©n nos ofrece herramientas para lidiar con ello. Herramientas como AWS Glue para jobs basados en Spark, S3 para almacenamiento, Kinesis para streaming y analytics, etc.\n",
    "\n",
    "\n",
    "##### Estructuras de datos, funciones integradas y librerias:\n",
    "\n",
    "  - Python ofrece una amplia variedad de funciones integradas que est√°n altamente optimizadas, por lo que es una buena pr√°ctica usarlas en cada escenario pertinente.\n",
    "  - Para los problemas presentados en este challenge, me pareci√≥ muy pertinente usar funciones como map(), reduce(), defaultdict(), Counter(). Adem√°s, integrarlas con el uso de generadores, ya que me permite hacer un uso muy eficiente de memoria.\n",
    "  - Existen librerias que facilitan muchisimo el trabajo y los calculos, sin embargo toca evaluar cuando es pertinente usar cada recurso. Como es el caso de pandas, que en la ultima secci√≥n hablar√© de ello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "\n",
    "- Las top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene por cada uno de esos d√≠as.\n",
    "\n",
    "\n",
    "   - Este problema tiene un grado de complejidad adicional, ya que se necesita un top_n anidado: primero uno para las fechas de los tweets y luego otro top_1 para el usuario de cada d√≠a.\n",
    "   - Este es un problema bastante com√∫n y siempre estar√° ligado al problema de ordenamiento, por lo que la t√≠pica soluci√≥n de recorrer los datos, ordenarlos y sacar los √∫ltimos elementos depender√° del algoritmo de ordenamiento usado.\n",
    "   - En el enfoque de tiempo de ejecuci√≥n, opt√© por implementar un mapper donde cada partici√≥n de datos termina en un top_10 local. Estos top_10 tienen anclados los usuarios que han interactuado cada d√≠a. Aqu√≠ debo decir que no es posible retornar un top_10 local y el top_1 calculado, ya que requiero la suma de todos los usuarios para poder hacer un buen reducer en el siguiente paso.\n",
    "   - Como reducer, se reciben N top_10 locales y simplemente se juntan por la llave fecha, es decir, se reduce a un top_10.\n",
    "   - Al final del algoritmo, cuando tengo solo 10 fechas, procedo a hacer el top_1 y retornar.\n",
    "   - Note que el problema del ordenamiento sigue impl√≠cito, simplemente que al ordenar sobre arreglos m√°s peque√±os y de forma paralela, la complejidad alcanza a reducirse.\n",
    "   - Como el ejercicio solo pide retornar la fecha junto con el usuario, me pareci√≥ que da un buen toque visual ordenar este top 10 por fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries to measure performance\n",
    "from cProfile import Profile\n",
    "from pstats import SortKey, Stats\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# functions used on challenge\n",
    "from q1_memory import q1_memory\n",
    "from q1_time import q1_time\n",
    "from q2_memory import q2_memory\n",
    "from q2_time import q2_time\n",
    "from q3_memory import q3_memory\n",
    "from q3_time import q3_time\n",
    "\n",
    "# general imports\n",
    "from statistics import mean\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# set file_path (same for all project)\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2021, 2, 12, 0, 0), 'RanbirS00614606'),\n",
      " (datetime.datetime(2021, 2, 13, 0, 0), 'MaanDee08215437'),\n",
      " (datetime.datetime(2021, 2, 14, 0, 0), 'rebelpacifist'),\n",
      " (datetime.datetime(2021, 2, 15, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 16, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 17, 0, 0), 'RaaJVinderkaur'),\n",
      " (datetime.datetime(2021, 2, 18, 0, 0), 'neetuanjle_nitu'),\n",
      " (datetime.datetime(2021, 2, 19, 0, 0), 'Preetm91'),\n",
      " (datetime.datetime(2021, 2, 20, 0, 0), 'MangalJ23056160'),\n",
      " (datetime.datetime(2021, 2, 23, 0, 0), 'Surrypuria')]\n",
      "\n",
      "\n",
      "Memory: 104.28 MB\n",
      "Time: 5.03 Segs\n"
     ]
    }
   ],
   "source": [
    "with Profile() as profile:\n",
    "    q1_memory_stat1 = memory_usage((lambda : pprint.pprint(q1_memory(file_path)),(), {}),interval=0.1)\n",
    "    q1_memory_stat2 = Stats(profile)\n",
    "    q1_memory_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "\n",
    "print(f\"\\n\\nMemory: {mean(q1_memory_stat1):.2f} MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q1_memory_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2021, 2, 12, 0, 0), 'RanbirS00614606'),\n",
      " (datetime.datetime(2021, 2, 13, 0, 0), 'MaanDee08215437'),\n",
      " (datetime.datetime(2021, 2, 14, 0, 0), 'rebelpacifist'),\n",
      " (datetime.datetime(2021, 2, 15, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 16, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 17, 0, 0), 'RaaJVinderkaur'),\n",
      " (datetime.datetime(2021, 2, 18, 0, 0), 'neetuanjle_nitu'),\n",
      " (datetime.datetime(2021, 2, 19, 0, 0), 'Preetm91'),\n",
      " (datetime.datetime(2021, 2, 20, 0, 0), 'MangalJ23056160'),\n",
      " (datetime.datetime(2021, 2, 23, 0, 0), 'Surrypuria')]\n",
      "\n",
      "\n",
      "Memory: 482.89MB\n",
      "Time: 4.88 Segs\n"
     ]
    }
   ],
   "source": [
    "with Profile() as profile:\n",
    "    q1_time_stat1 = memory_usage((lambda : pprint.pprint(q1_time(file_path)),(), {}),interval=0.1)\n",
    "    q1_time_stat2 = Stats(profile)\n",
    "    q1_time_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "\n",
    "print(f\"\\n\\nMemory: {mean(q1_time_stat1):.2f}MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q1_time_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "\n",
    "- Los top 10 emojis m√°s usados con su respectivo conteo.\n",
    "\n",
    "\n",
    "   - Este ejercicio tambi√©n involucra obtener un top N, solo que este es m√°s sencillo (no es anidado).\n",
    "   - Como mencion√© anteriormente, el problema de obtener un top N est√° muy relacionado con la complejidad en el ordenamiento.\n",
    "   - La complejidad aqu√≠ radica en el hecho de reconocer emojis, ya que no es tarea f√°cil debido a la infinidad que existe. Sabemos que vienen codificados en Unicode y conocemos algunos, pero ¬øc√≥mo obtener absolutamente todos los emojis y no dejar ninguno por fuera?\n",
    "     - Para esto me apoy√© en la librer√≠a emoji, que es muy efectiva en el tratamiento de este tipo de caracteres. Aqu√≠ encontr√© que buscar emojis en un texto puede llegar a ser costoso, y que esta libreria lo hace mediante √°rboles.\n",
    "     - Sin embargo, me pareci√≥ una buena oportunidad para probar el poder del paralelismo, ya que al ser una tarea m√°s exhaustiva que los otros problemas, tener 2 o 3 particiones haci√©ndolo al mismo tiempo trae sus beneficios. Aqu√≠ vemos que el tiempo de ejecuci√≥n es casi la mitad que el tiempo de ejecuci√≥n lineal.\n",
    "     - Es posible usar otra aproximaci√≥n al problema y hacerlo a√∫n m√°s r√°pido, como utilizar diccionarios y expresiones regulares para reconocer los emojis. Sin embargo, podr√≠amos estar perdiendo precisi√≥n en el conteo, ya que algunos emojis podr√≠an no reconocerse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 5049),\n",
      " ('üòÇ', 3072),\n",
      " ('üöú', 2972),\n",
      " ('üåæ', 2182),\n",
      " ('üáÆüá≥', 2086),\n",
      " ('ü§£', 1668),\n",
      " ('‚úä', 1651),\n",
      " ('‚ù§Ô∏è', 1382),\n",
      " ('üôèüèª', 1317),\n",
      " ('üíö', 1040)]\n",
      "\n",
      "\n",
      "Memory: 185.68MB\n",
      "Time: 31.59 Segs\n"
     ]
    }
   ],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "with Profile() as profile:\n",
    "    q2_memory_stat1 = memory_usage((lambda : pprint.pprint(q2_memory(file_path)),(), {}),interval=0.1)\n",
    "    q2_memory_stat2 = Stats(profile)\n",
    "    q2_memory_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "\n",
    "print(f\"\\n\\nMemory: {mean(q2_memory_stat1):.2f}MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q2_memory_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 5049),\n",
      " ('üòÇ', 3072),\n",
      " ('üöú', 2972),\n",
      " ('üåæ', 2182),\n",
      " ('üáÆüá≥', 2086),\n",
      " ('ü§£', 1668),\n",
      " ('‚úä', 1651),\n",
      " ('‚ù§Ô∏è', 1382),\n",
      " ('üôèüèª', 1317),\n",
      " ('üíö', 1040)]\n",
      "\n",
      "\n",
      "Memory: 534.25MB\n",
      "Time: 18.22 Segs\n"
     ]
    }
   ],
   "source": [
    "with Profile() as profile:\n",
    "    q2_time_stat1 = memory_usage((lambda : pprint.pprint(q2_time(file_path)),(), {}),interval=0.1)\n",
    "    q2_time_stat2 = Stats(profile)\n",
    "    q2_time_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "\n",
    "print(f\"\\n\\nMemory: {mean(q2_time_stat1):.2f}MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q2_time_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3\n",
    "- El top 10 hist√≥rico de usuarios (username) m√°s influyentes en funci√≥n del conteo de las menciones (@) que registra cada uno de ellos.\n",
    "\n",
    "   - Este problema es muy an√°logo al anterior.\n",
    "   - La complejidad aqu√≠ radica en reconocer los nombres de usuario para la plataforma de Twitter:\n",
    "     - Entre 4 y 15 caracteres.\n",
    "     - No reconoce entre may√∫sculas y min√∫sculas.\n",
    "     - Solo acepta caracteres alfanum√©ricos y guion bajo (en cualquier orden).\n",
    "   - Me apoy√© en el m√≥dulo integrado de Python \"re\", este m√≥dulo es incre√≠blemente eficiente para trabajar con expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NARENDRAMODI', 2266),\n",
      " ('KISANEKTAMORCHA', 1840),\n",
      " ('RAKESHTIKAITBKU', 1642),\n",
      " ('PMOINDIA', 1427),\n",
      " ('RAHULGANDHI', 1146),\n",
      " ('GRETATHUNBERG', 1048),\n",
      " ('RAVISINGHKA', 1019),\n",
      " ('RIHANNA', 986),\n",
      " ('UNHUMANRIGHTS', 962),\n",
      " ('MEENAHARRIS', 926)]\n",
      "\n",
      "\n",
      "Memory: 194.82MB\n",
      "Time: 6.10 Segs\n"
     ]
    }
   ],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "with Profile() as profile:\n",
    "    q3_memory_stat1 = memory_usage((lambda : pprint.pprint(q3_memory(file_path)),(), {}),interval=0.1)\n",
    "    q3_memory_stat2 = Stats(profile)\n",
    "    q3_memory_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "print(f\"\\n\\nMemory: {mean(q3_memory_stat1):.2f}MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q3_memory_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NARENDRAMODI', 2266),\n",
      " ('KISANEKTAMORCHA', 1840),\n",
      " ('RAKESHTIKAITBKU', 1642),\n",
      " ('PMOINDIA', 1427),\n",
      " ('RAHULGANDHI', 1146),\n",
      " ('GRETATHUNBERG', 1048),\n",
      " ('RAVISINGHKA', 1019),\n",
      " ('RIHANNA', 986),\n",
      " ('UNHUMANRIGHTS', 962),\n",
      " ('MEENAHARRIS', 926)]\n",
      "\n",
      "\n",
      "Memory: 488.95MB\n",
      "Time: 5.03 Segs\n"
     ]
    }
   ],
   "source": [
    "with Profile() as profile:\n",
    "    q3_time_stat1 = memory_usage((lambda : pprint.pprint(q3_time(file_path)),(), {}),interval=0.1)\n",
    "    q3_time_stat2 = Stats(profile)\n",
    "    q3_time_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "print(f\"\\n\\nMemory: {mean(q3_time_stat1):.2f}MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q3_time_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "- Pandas es justamente una libreria totalmente optimizada para el manejo y transformaci√≥n de datos, por esta raz√≥n quiero mostrar un peque√±o ejemplo de como se solucionaria Q1 usando esta libreria.\n",
    "\n",
    "- Adicional, quiero que veamos algunas consideraciones:\n",
    "  - pandas es extremadamente habil haciendo transformaci√≥n de datos gracias a su capacidad de tener todos los datos cargados en memoria sin importar si se est√° usando o no. Esto tiene vetajas como los 200ms obtenidos solamente al hacer la agrupaci√≥n requerida en Q1.\n",
    "  - El hecho de cargar todo en memoria lo hace muy optimo en comlejidad computacional, pero no lo hace nada optimo en uso de memoria, esto se refleja en el tiempo tan extenso que toma cargar todos los datos (400mb approx). Solamente cargando el dataset, demoramos 10 segundos, m√°s que la soluci√≥n completa presentada en la primera secci√≥n, y adem√°s, solo el dataset, si hacer transformaciones a√∫n, consume approx 280Mb en memoria.\n",
    "  - Tambi√©n es cierto que pandas nos deja hacer cargues a manera de batch, es decir, podemos partir este archivo en x dataframes durante el cargue, y esto optimiza mucho los tiempos, sin embargo no quita el hecho de que seguir√° todo cargado en memoria.\n",
    "  - Por otro lado, es nuestra tarea que apenas se carga el datframe, eliminar todo lo que no sea necesario, y dejar cargado lo m√≠nimo posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117407 entries, 0 to 117406\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   url              117407 non-null  object \n",
      " 1   date             117407 non-null  object \n",
      " 2   content          117407 non-null  object \n",
      " 3   renderedContent  117407 non-null  object \n",
      " 4   id               117407 non-null  int64  \n",
      " 5   user             117407 non-null  object \n",
      " 6   outlinks         117407 non-null  object \n",
      " 7   tcooutlinks      117407 non-null  object \n",
      " 8   replyCount       117407 non-null  int64  \n",
      " 9   retweetCount     117407 non-null  int64  \n",
      " 10  likeCount        117407 non-null  int64  \n",
      " 11  quoteCount       117407 non-null  int64  \n",
      " 12  conversationId   117407 non-null  int64  \n",
      " 13  lang             117407 non-null  object \n",
      " 14  source           117407 non-null  object \n",
      " 15  sourceUrl        116495 non-null  object \n",
      " 16  sourceLabel      116495 non-null  object \n",
      " 17  media            28109 non-null   object \n",
      " 18  retweetedTweet   0 non-null       float64\n",
      " 19  quotedTweet      41436 non-null   object \n",
      " 20  mentionedUsers   38034 non-null   object \n",
      "dtypes: float64(1), int64(6), object(14)\n",
      "memory usage: 274.3 MB\n",
      "None\n",
      "CPU times: user 8.6 s, sys: 1.69 s, total: 10.3 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Leo el archivo, le indico que cada linea es un json, y que las fechas las deje en string\n",
    "df = pd.read_json(file_path, lines=True, convert_dates=False)\n",
    "\n",
    "# Selecciono solamente las dos columnas que necesito, lo dem√°s se pueder ir para liberrar memoria\n",
    "df_selected = df[['date', 'user']]\n",
    "\n",
    "# Obtener informaci√≥n sobre el DataFrame\n",
    "info = df.info(memory_usage='deep')\n",
    "\n",
    "# Elimino el dataframe original, ya que es excesivamente grande\n",
    "del df\n",
    "\n",
    "# Aplico la transformaci√≥n del usuario, obtengo el username y me deshago de todo lo dem\n",
    "df_selected['user'] = df_selected['user'].apply(lambda x: x['username'])\n",
    "\n",
    "# Aplico la transformaci√≥n de la fecha, para quedarme solo con AAAA-MM-DD, esto facilita los agrupamientos\n",
    "df_selected['date'] = df_selected['date'].apply(lambda x: x[:10])\n",
    "\n",
    "# Imprimir la informaci√≥n\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2021, 2, 12, 0, 0), 'RanbirS00614606'),\n",
      " (datetime.datetime(2021, 2, 13, 0, 0), 'MaanDee08215437'),\n",
      " (datetime.datetime(2021, 2, 14, 0, 0), 'rebelpacifist'),\n",
      " (datetime.datetime(2021, 2, 15, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 16, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 17, 0, 0), 'RaaJVinderkaur'),\n",
      " (datetime.datetime(2021, 2, 18, 0, 0), 'neetuanjle_nitu'),\n",
      " (datetime.datetime(2021, 2, 19, 0, 0), 'Preetm91'),\n",
      " (datetime.datetime(2021, 2, 20, 0, 0), 'MangalJ23056160'),\n",
      " (datetime.datetime(2021, 2, 23, 0, 0), 'Surrypuria')]\n",
      "CPU times: user 155 ms, sys: 20.8 ms, total: 176 ms\n",
      "Wall time: 175 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Primera Agrupaci√≥n de conteo por fecha. Esto nos da el conteo de tweets cada d√≠a.\n",
    "# Observe que se resetean los indices para que el agrupamiento de conteo se vuelva una columna llamada \"count\"\n",
    "group = df_selected.groupby([\"date\"]).size().reset_index(name=\"count\").nlargest(10, \"count\")\n",
    "\n",
    "# Se filtra el df principal para deshacernos de las fechas que ya no est√°n en el top\n",
    "df_selected = df_selected[df_selected[\"date\"].isin(group[\"date\"])]\n",
    "del group\n",
    "\n",
    "# Segunda Agrupaci√≥n de conteo por fecha y usuario al tiempo. Esto nos da el conteo de cada usuario en cada d√≠a.\n",
    "# Observe que se resetean los idices para volver a un dataframe con solo dos indices, el agrupamiento de conteo se vuelve una columna llamada \"count\"\n",
    "group2 = df_selected.groupby([\"date\",\"user\"]).size().reset_index(name='count')\n",
    "\n",
    "# Ya teniendo el conteo de cada usuario en cada d√≠a, volvemos a agrupar por fecha, esta vez quedadoos con aque usuario que tuvo m√°s conteo, es decir, aquel que hizo m√°s publicacioes cada d√≠a\n",
    "group2 = group2.groupby(\"date\").apply(lambda groupdate: groupdate.nlargest(1, 'count'))\n",
    "\n",
    "# este agrupamiento nos da el usuario m√°s activo cada dia, juto con el indice en el que se encontraba\n",
    "group2 = group2.reset_index(drop=True)\n",
    "group2\n",
    "pprint.pprint([(datetime.fromisoformat(group2[\"date\"][i]), group2[\"user\"][i]) for i in range(len(group2))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
