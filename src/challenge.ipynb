{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge - David Felipe Martinez Castiblanco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalidades\n",
    "\n",
    "A continuación, presentaré una serie de generalidades que aplican para todas las preguntas:\n",
    "\n",
    "##### Estilo de código\n",
    "  - Usé pylint y pycodestyle para aumentar la calidad de mi código, sin embargo desactivé dos reglas:\n",
    "     - R0801: Existe código duplicasdo entre archivos, la razón es que estoy usando estrategias análogas en los archivos, soy conciente que es una mala practica y el código deberia ser mucho más modular, sin embargo etendí que el ejercicio no lo permite.\n",
    "     - W1514: Estoy abriendo archivos sin especificar su codificación, la razón de desactivarlo es que no conozco la codificación original del archivo, y no quiero depronto dañar mis calculos por una mala codificació, esta vez le dejo a python buscar la correcta.\n",
    "\n",
    "##### Estructura del Dataset:\n",
    "  - Es un dataset excesivamente grande en comparación a lo que se requiere de este. Solamente se usaron 3 atributos de todos los que tiene cada muestra.\n",
    "  - Debido a que los atributos requeridos no tenian inconsistencias, no se hizo limpieza alguna del dataset, exceptuando la limpieza minima de cada ejercicio (pasar a mayusculas un usuario, extraer substrings de algún texto).\n",
    "  - Vi que varios tweets tenian otros tweets anidados (quoted), sin embargo hice la suposición de que no se tendrán en cuenta para los conteos, es decir, no están incluidos en ninguna parte de mi ejercicio.\n",
    "\n",
    "##### Lectura de los datos:\n",
    "\n",
    "  - Estamos hablando de un archivo algo pesado. Si pensamos en cargarlo por completo en memoria, tendríamos que aprovecharlo muy bien y empezar a paralelizar tareas. Este sería el caso cuando queremos optimizar el tiempo de ejecución.\n",
    "  - Si tenemos los datos divididos desde un principio, podríamos hacer que múltiples hilos abran cada partición y la procesen. En nuestro caso, tenemos un archivo completo, pero podemos cargarlo todo en memoria, particionarlo en chunks y hacer cálculos de manera paralela.\n",
    "     - Entendí que el ejercicio no me permite usar más funciones, pero en una futura mejora, veo obligatorio particionar el archivo y limpiarlo para dejar solo los atributos que se necesitan. Esto facilita y optimiza el proceso de trasnformación y obtención de metricas\n",
    "  - En los 3 ejercicios enfocados en tiempo de procesamiento, los llevé hacia el enfoque map-reduce, y por medio de multihilo procesé lo que se requería. Sé que no es tan óptimo cuando se presentan pocos datos o un solo archivo, pero cuando la cantidad de datos tiende crece de manera masiva(big data), esta solución superaría por mucho al enfoque que di optimizando memoria.\n",
    "  - Por el contrario, si necesitamos optimizar el uso de memoria, podemos ir leyendo los datos línea a línea y calculando secuencialmente lo que se requiere. Esto se convierte en una tarea lineal, pero es una manera de cargar un archivo pesado optimizando el uso de memoria.\n",
    "  - Está claro que al escalar el problema, es decir, si tenemos x archivos del mismo o mayor tamaño, esta solución se vuelve insostenible. Por lo que debe escalar hacia algo enfocado en arquitectura big data. Python, por ejemplo, nos ofrece librerías como MRJob para esto.\n",
    "  - Siempre es útil liberar memoria dinamicamente, es decir, borrar aquellas variables pesadas que se usan una sola vez.\n",
    "  - A gran escala, el cargue de los datos es tan importante como su procesamiento, y la nube también nos ofrece herramientas para lidiar con ello. Herramientas como AWS Glue para jobs basados en Spark, S3 para almacenamiento, Kinesis para streaming y analytics, etc.\n",
    "\n",
    "\n",
    "##### Estructuras de datos, funciones integradas y librerias:\n",
    "\n",
    "  - Python ofrece una amplia variedad de funciones integradas que están altamente optimizadas, por lo que es una buena práctica usarlas en cada escenario pertinente.\n",
    "  - Para los problemas presentados en este challenge, me pareció muy pertinente usar funciones como map(), reduce(), defaultdict(), Counter(). Además, integrarlas con el uso de generadores, ya que me permite hacer un uso muy eficiente de memoria.\n",
    "  - Existen librerias que facilitan muchisimo el trabajo y los calculos, sin embargo toca evaluar cuando es pertinente usar cada recurso. Como es el caso de pandas, que en la ultima sección hablaré de ello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "\n",
    "- Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días.\n",
    "\n",
    "\n",
    "   - Este problema tiene un grado de complejidad adicional, ya que se necesita un top_n anidado: primero uno para las fechas de los tweets y luego otro top_1 para el usuario de cada día.\n",
    "   - Este es un problema bastante común y siempre estará ligado al problema de ordenamiento, por lo que la típica solución de recorrer los datos, ordenarlos y sacar los últimos elementos dependerá del algoritmo de ordenamiento usado.\n",
    "   - En el enfoque de tiempo de ejecución, opté por implementar un mapper donde cada partición de datos termina en un top_10 local. Estos top_10 tienen anclados los usuarios que han interactuado cada día. Aquí debo decir que no es posible retornar un top_10 local y el top_1 calculado, ya que requiero la suma de todos los usuarios para poder hacer un buen reducer en el siguiente paso.\n",
    "   - Como reducer, se reciben N top_10 locales y simplemente se juntan por la llave fecha, es decir, se reduce a un top_10.\n",
    "   - Al final del algoritmo, cuando tengo solo 10 fechas, procedo a hacer el top_1 y retornar.\n",
    "   - Note que el problema del ordenamiento sigue implícito, simplemente que al ordenar sobre arreglos más pequeños y de forma paralela, la complejidad alcanza a reducirse.\n",
    "   - Como el ejercicio solo pide retornar la fecha junto con el usuario, me pareció que da un buen toque visual ordenar este top 10 por fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries to measure performance\n",
    "from cProfile import Profile\n",
    "from pstats import SortKey, Stats\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# functions used on challenge\n",
    "from q1_memory import q1_memory\n",
    "from q1_time import q1_time\n",
    "from q2_memory import q2_memory\n",
    "from q2_time import q2_time\n",
    "from q3_memory import q3_memory\n",
    "from q3_time import q3_time\n",
    "\n",
    "# general imports\n",
    "from statistics import mean\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# set file_path (same for all project)\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2021, 2, 12, 0, 0), 'RanbirS00614606'),\n",
      " (datetime.datetime(2021, 2, 13, 0, 0), 'MaanDee08215437'),\n",
      " (datetime.datetime(2021, 2, 14, 0, 0), 'rebelpacifist'),\n",
      " (datetime.datetime(2021, 2, 15, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 16, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 17, 0, 0), 'RaaJVinderkaur'),\n",
      " (datetime.datetime(2021, 2, 18, 0, 0), 'neetuanjle_nitu'),\n",
      " (datetime.datetime(2021, 2, 19, 0, 0), 'Preetm91'),\n",
      " (datetime.datetime(2021, 2, 20, 0, 0), 'MangalJ23056160'),\n",
      " (datetime.datetime(2021, 2, 23, 0, 0), 'Surrypuria')]\n",
      "\n",
      "\n",
      "Memory: 104.28 MB\n",
      "Time: 5.03 Segs\n"
     ]
    }
   ],
   "source": [
    "with Profile() as profile:\n",
    "    q1_memory_stat1 = memory_usage((lambda : pprint.pprint(q1_memory(file_path)),(), {}),interval=0.1)\n",
    "    q1_memory_stat2 = Stats(profile)\n",
    "    q1_memory_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "\n",
    "print(f\"\\n\\nMemory: {mean(q1_memory_stat1):.2f} MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q1_memory_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2021, 2, 12, 0, 0), 'RanbirS00614606'),\n",
      " (datetime.datetime(2021, 2, 13, 0, 0), 'MaanDee08215437'),\n",
      " (datetime.datetime(2021, 2, 14, 0, 0), 'rebelpacifist'),\n",
      " (datetime.datetime(2021, 2, 15, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 16, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 17, 0, 0), 'RaaJVinderkaur'),\n",
      " (datetime.datetime(2021, 2, 18, 0, 0), 'neetuanjle_nitu'),\n",
      " (datetime.datetime(2021, 2, 19, 0, 0), 'Preetm91'),\n",
      " (datetime.datetime(2021, 2, 20, 0, 0), 'MangalJ23056160'),\n",
      " (datetime.datetime(2021, 2, 23, 0, 0), 'Surrypuria')]\n",
      "\n",
      "\n",
      "Memory: 482.89MB\n",
      "Time: 4.88 Segs\n"
     ]
    }
   ],
   "source": [
    "with Profile() as profile:\n",
    "    q1_time_stat1 = memory_usage((lambda : pprint.pprint(q1_time(file_path)),(), {}),interval=0.1)\n",
    "    q1_time_stat2 = Stats(profile)\n",
    "    q1_time_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "\n",
    "print(f\"\\n\\nMemory: {mean(q1_time_stat1):.2f}MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q1_time_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "\n",
    "- Los top 10 emojis más usados con su respectivo conteo.\n",
    "\n",
    "\n",
    "   - Este ejercicio también involucra obtener un top N, solo que este es más sencillo (no es anidado).\n",
    "   - Como mencioné anteriormente, el problema de obtener un top N está muy relacionado con la complejidad en el ordenamiento.\n",
    "   - La complejidad aquí radica en el hecho de reconocer emojis, ya que no es tarea fácil debido a la infinidad que existe. Sabemos que vienen codificados en Unicode y conocemos algunos, pero ¿cómo obtener absolutamente todos los emojis y no dejar ninguno por fuera?\n",
    "     - Para esto me apoyé en la librería emoji, que es muy efectiva en el tratamiento de este tipo de caracteres. Aquí encontré que buscar emojis en un texto puede llegar a ser costoso, y que esta libreria lo hace mediante árboles.\n",
    "     - Sin embargo, me pareció una buena oportunidad para probar el poder del paralelismo, ya que al ser una tarea más exhaustiva que los otros problemas, tener 2 o 3 particiones haciéndolo al mismo tiempo trae sus beneficios. Aquí vemos que el tiempo de ejecución es casi la mitad que el tiempo de ejecución lineal.\n",
    "     - Es posible usar otra aproximación al problema y hacerlo aún más rápido, como utilizar diccionarios y expresiones regulares para reconocer los emojis. Sin embargo, podríamos estar perdiendo precisión en el conteo, ya que algunos emojis podrían no reconocerse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 5049),\n",
      " ('😂', 3072),\n",
      " ('🚜', 2972),\n",
      " ('🌾', 2182),\n",
      " ('🇮🇳', 2086),\n",
      " ('🤣', 1668),\n",
      " ('✊', 1651),\n",
      " ('❤️', 1382),\n",
      " ('🙏🏻', 1317),\n",
      " ('💚', 1040)]\n",
      "\n",
      "\n",
      "Memory: 185.68MB\n",
      "Time: 31.59 Segs\n"
     ]
    }
   ],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "with Profile() as profile:\n",
    "    q2_memory_stat1 = memory_usage((lambda : pprint.pprint(q2_memory(file_path)),(), {}),interval=0.1)\n",
    "    q2_memory_stat2 = Stats(profile)\n",
    "    q2_memory_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "\n",
    "print(f\"\\n\\nMemory: {mean(q2_memory_stat1):.2f}MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q2_memory_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 5049),\n",
      " ('😂', 3072),\n",
      " ('🚜', 2972),\n",
      " ('🌾', 2182),\n",
      " ('🇮🇳', 2086),\n",
      " ('🤣', 1668),\n",
      " ('✊', 1651),\n",
      " ('❤️', 1382),\n",
      " ('🙏🏻', 1317),\n",
      " ('💚', 1040)]\n",
      "\n",
      "\n",
      "Memory: 534.25MB\n",
      "Time: 18.22 Segs\n"
     ]
    }
   ],
   "source": [
    "with Profile() as profile:\n",
    "    q2_time_stat1 = memory_usage((lambda : pprint.pprint(q2_time(file_path)),(), {}),interval=0.1)\n",
    "    q2_time_stat2 = Stats(profile)\n",
    "    q2_time_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "\n",
    "print(f\"\\n\\nMemory: {mean(q2_time_stat1):.2f}MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q2_time_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3\n",
    "- El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos.\n",
    "\n",
    "   - Este problema es muy análogo al anterior.\n",
    "   - La complejidad aquí radica en reconocer los nombres de usuario para la plataforma de Twitter:\n",
    "     - Entre 4 y 15 caracteres.\n",
    "     - No reconoce entre mayúsculas y minúsculas.\n",
    "     - Solo acepta caracteres alfanuméricos y guion bajo (en cualquier orden).\n",
    "   - Me apoyé en el módulo integrado de Python \"re\", este módulo es increíblemente eficiente para trabajar con expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NARENDRAMODI', 2266),\n",
      " ('KISANEKTAMORCHA', 1840),\n",
      " ('RAKESHTIKAITBKU', 1642),\n",
      " ('PMOINDIA', 1427),\n",
      " ('RAHULGANDHI', 1146),\n",
      " ('GRETATHUNBERG', 1048),\n",
      " ('RAVISINGHKA', 1019),\n",
      " ('RIHANNA', 986),\n",
      " ('UNHUMANRIGHTS', 962),\n",
      " ('MEENAHARRIS', 926)]\n",
      "\n",
      "\n",
      "Memory: 194.82MB\n",
      "Time: 6.10 Segs\n"
     ]
    }
   ],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "with Profile() as profile:\n",
    "    q3_memory_stat1 = memory_usage((lambda : pprint.pprint(q3_memory(file_path)),(), {}),interval=0.1)\n",
    "    q3_memory_stat2 = Stats(profile)\n",
    "    q3_memory_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "print(f\"\\n\\nMemory: {mean(q3_memory_stat1):.2f}MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q3_memory_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NARENDRAMODI', 2266),\n",
      " ('KISANEKTAMORCHA', 1840),\n",
      " ('RAKESHTIKAITBKU', 1642),\n",
      " ('PMOINDIA', 1427),\n",
      " ('RAHULGANDHI', 1146),\n",
      " ('GRETATHUNBERG', 1048),\n",
      " ('RAVISINGHKA', 1019),\n",
      " ('RIHANNA', 986),\n",
      " ('UNHUMANRIGHTS', 962),\n",
      " ('MEENAHARRIS', 926)]\n",
      "\n",
      "\n",
      "Memory: 488.95MB\n",
      "Time: 5.03 Segs\n"
     ]
    }
   ],
   "source": [
    "with Profile() as profile:\n",
    "    q3_time_stat1 = memory_usage((lambda : pprint.pprint(q3_time(file_path)),(), {}),interval=0.1)\n",
    "    q3_time_stat2 = Stats(profile)\n",
    "    q3_time_stat2.strip_dirs()\n",
    "    #.sort_stats(SortKey.CUMULATIVE)\n",
    "    #.print_stats(10)\n",
    "print(f\"\\n\\nMemory: {mean(q3_time_stat1):.2f}MB\")\n",
    "print(f\"Time: {sum(stat[2] for stat in q3_time_stat2.stats.values()):.2f} Segs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "- Pandas es justamente una libreria totalmente optimizada para el manejo y transformación de datos, por esta razón quiero mostrar un pequeño ejemplo de como se solucionaria Q1 usando esta libreria.\n",
    "\n",
    "- Adicional, quiero que veamos algunas consideraciones:\n",
    "  - pandas es extremadamente habil haciendo transformación de datos gracias a su capacidad de tener todos los datos cargados en memoria sin importar si se está usando o no. Esto tiene vetajas como los 200ms obtenidos solamente al hacer la agrupación requerida en Q1.\n",
    "  - El hecho de cargar todo en memoria lo hace muy optimo en comlejidad computacional, pero no lo hace nada optimo en uso de memoria, esto se refleja en el tiempo tan extenso que toma cargar todos los datos (400mb approx). Solamente cargando el dataset, demoramos 10 segundos, más que la solución completa presentada en la primera sección, y además, solo el dataset, si hacer transformaciones aún, consume approx 280Mb en memoria.\n",
    "  - También es cierto que pandas nos deja hacer cargues a manera de batch, es decir, podemos partir este archivo en x dataframes durante el cargue, y esto optimiza mucho los tiempos, sin embargo no quita el hecho de que seguirá todo cargado en memoria.\n",
    "  - Por otro lado, es nuestra tarea que apenas se carga el datframe, eliminar todo lo que no sea necesario, y dejar cargado lo mínimo posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117407 entries, 0 to 117406\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   url              117407 non-null  object \n",
      " 1   date             117407 non-null  object \n",
      " 2   content          117407 non-null  object \n",
      " 3   renderedContent  117407 non-null  object \n",
      " 4   id               117407 non-null  int64  \n",
      " 5   user             117407 non-null  object \n",
      " 6   outlinks         117407 non-null  object \n",
      " 7   tcooutlinks      117407 non-null  object \n",
      " 8   replyCount       117407 non-null  int64  \n",
      " 9   retweetCount     117407 non-null  int64  \n",
      " 10  likeCount        117407 non-null  int64  \n",
      " 11  quoteCount       117407 non-null  int64  \n",
      " 12  conversationId   117407 non-null  int64  \n",
      " 13  lang             117407 non-null  object \n",
      " 14  source           117407 non-null  object \n",
      " 15  sourceUrl        116495 non-null  object \n",
      " 16  sourceLabel      116495 non-null  object \n",
      " 17  media            28109 non-null   object \n",
      " 18  retweetedTweet   0 non-null       float64\n",
      " 19  quotedTweet      41436 non-null   object \n",
      " 20  mentionedUsers   38034 non-null   object \n",
      "dtypes: float64(1), int64(6), object(14)\n",
      "memory usage: 274.3 MB\n",
      "None\n",
      "CPU times: user 8.6 s, sys: 1.69 s, total: 10.3 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Leo el archivo, le indico que cada linea es un json, y que las fechas las deje en string\n",
    "df = pd.read_json(file_path, lines=True, convert_dates=False)\n",
    "\n",
    "# Selecciono solamente las dos columnas que necesito, lo demás se pueder ir para liberrar memoria\n",
    "df_selected = df[['date', 'user']]\n",
    "\n",
    "# Obtener información sobre el DataFrame\n",
    "info = df.info(memory_usage='deep')\n",
    "\n",
    "# Elimino el dataframe original, ya que es excesivamente grande\n",
    "del df\n",
    "\n",
    "# Aplico la transformación del usuario, obtengo el username y me deshago de todo lo dem\n",
    "df_selected['user'] = df_selected['user'].apply(lambda x: x['username'])\n",
    "\n",
    "# Aplico la transformación de la fecha, para quedarme solo con AAAA-MM-DD, esto facilita los agrupamientos\n",
    "df_selected['date'] = df_selected['date'].apply(lambda x: x[:10])\n",
    "\n",
    "# Imprimir la información\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2021, 2, 12, 0, 0), 'RanbirS00614606'),\n",
      " (datetime.datetime(2021, 2, 13, 0, 0), 'MaanDee08215437'),\n",
      " (datetime.datetime(2021, 2, 14, 0, 0), 'rebelpacifist'),\n",
      " (datetime.datetime(2021, 2, 15, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 16, 0, 0), 'jot__b'),\n",
      " (datetime.datetime(2021, 2, 17, 0, 0), 'RaaJVinderkaur'),\n",
      " (datetime.datetime(2021, 2, 18, 0, 0), 'neetuanjle_nitu'),\n",
      " (datetime.datetime(2021, 2, 19, 0, 0), 'Preetm91'),\n",
      " (datetime.datetime(2021, 2, 20, 0, 0), 'MangalJ23056160'),\n",
      " (datetime.datetime(2021, 2, 23, 0, 0), 'Surrypuria')]\n",
      "CPU times: user 155 ms, sys: 20.8 ms, total: 176 ms\n",
      "Wall time: 175 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Primera Agrupación de conteo por fecha. Esto nos da el conteo de tweets cada día.\n",
    "# Observe que se resetean los indices para que el agrupamiento de conteo se vuelva una columna llamada \"count\"\n",
    "group = df_selected.groupby([\"date\"]).size().reset_index(name=\"count\").nlargest(10, \"count\")\n",
    "\n",
    "# Se filtra el df principal para deshacernos de las fechas que ya no están en el top\n",
    "df_selected = df_selected[df_selected[\"date\"].isin(group[\"date\"])]\n",
    "del group\n",
    "\n",
    "# Segunda Agrupación de conteo por fecha y usuario al tiempo. Esto nos da el conteo de cada usuario en cada día.\n",
    "# Observe que se resetean los idices para volver a un dataframe con solo dos indices, el agrupamiento de conteo se vuelve una columna llamada \"count\"\n",
    "group2 = df_selected.groupby([\"date\",\"user\"]).size().reset_index(name='count')\n",
    "\n",
    "# Ya teniendo el conteo de cada usuario en cada día, volvemos a agrupar por fecha, esta vez quedadoos con aque usuario que tuvo más conteo, es decir, aquel que hizo más publicacioes cada día\n",
    "group2 = group2.groupby(\"date\").apply(lambda groupdate: groupdate.nlargest(1, 'count'))\n",
    "\n",
    "# este agrupamiento nos da el usuario más activo cada dia, juto con el indice en el que se encontraba\n",
    "group2 = group2.reset_index(drop=True)\n",
    "group2\n",
    "pprint.pprint([(datetime.fromisoformat(group2[\"date\"][i]), group2[\"user\"][i]) for i in range(len(group2))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
